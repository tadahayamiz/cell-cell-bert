{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# PubMedBERT CPT (MLM only) + RAdam + loss logging\n",
    "# + Validation Loss & Early Stopping\n",
    "# ==============================\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,  # <--- 追加\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sqlite3\n",
    "\n",
    "# ---- パラメータ探索設定 ----\n",
    "param_grid = {\n",
    "    \"learning_rate\": [2e-05],\n",
    "    \"weight_decay\": [0.0001],\n",
    "    \"warmup_ratio\": [0.1],\n",
    "}\n",
    "\n",
    "# ---- 学習ループ ----\n",
    "for lr, wd, warmup in itertools.product(\n",
    "    param_grid[\"learning_rate\"],\n",
    "    param_grid[\"weight_decay\"],\n",
    "    param_grid[\"warmup_ratio\"]\n",
    "):\n",
    "\n",
    "    # ---- トークナイザの準備 ----\n",
    "    model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    tokenizer.add_tokens([\"[CELL0]\", \"[CELL1]\"])\n",
    "\n",
    "    cpt_path = \"../../00_pubmed_ext_2cells/data_for_annotation.csv\"\n",
    "    db_file_path = \"../../00_pubmed_ext_2cells/hit_cells.db\"\n",
    "\n",
    "    conn = sqlite3.connect(db_file_path)\n",
    "    query = \"SELECT pmid, sent_id, ann_sentence as sentence FROM hit_cells\" \n",
    "    db_df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    db_df['sentence'] = db_df['sentence'].str.replace(r\"[【】]\", \"\", regex=True)\n",
    "    text_lookup_dict = dict(zip(zip(db_df['pmid'], db_df['sent_id']), db_df['sentence']))\n",
    "\n",
    "    # --- データ読み込み ---\n",
    "    cpt_df = pd.read_csv(cpt_path)\n",
    "\n",
    "    indices_to_modify = cpt_df.sample(frac=0.5, random_state=42).index\n",
    "\n",
    "    def get_db_text(row):\n",
    "        key = (row['pmid'], row['sent_id'])\n",
    "        # DBに存在すればそのテキスト（【 or】削除済み）を返す。なければ元のテキストを維持\n",
    "        return text_lookup_dict.get(key, row['Cleaned_Sentence'])\n",
    "\n",
    "    new_sentences = cpt_df.loc[indices_to_modify].apply(get_db_text, axis=1)\n",
    "    cpt_df.loc[indices_to_modify, 'Cleaned_Sentence'] = new_sentences\n",
    "\n",
    "    print(cpt_df.head())\n",
    "\n",
    "    dataset = Dataset.from_pandas(cpt_df)\n",
    "\n",
    "    # --- トークナイズ ---\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"Cleaned_Sentence\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "    tokenized_dataset_full = dataset.map(  # <--- 変更 (変数名)\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"pmid\",\"sent_id\",\"ID_A\",\"ID_B\",\"Selected_Term_A\",\"Selected_Term_B\",\"Cleaned_Sentence\"]\n",
    "    )\n",
    "\n",
    "    # --- 検証用データセットの分割 (95% train, 5% eval) --- # <--- 追加\n",
    "    split_dataset = tokenized_dataset_full.train_test_split(test_size=0.05, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "    print(f\"Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}\")\n",
    "    # --- ここまで追加 ---\n",
    "\n",
    "    # ---- データコラトラ ----\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=True,\n",
    "        mlm_probability=0.15\n",
    "    )\n",
    "\n",
    "    exp_name = f\"lr{lr}_wd{wd}_warmup{warmup}\"\n",
    "    output_dir = f\"./pretrain_half/{exp_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ---- モデル読み込み ----\n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # ---- 早期停止コールバック ---- # <--- 追加\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5,  # 5回連続でeval_lossが改善しなかったら停止\n",
    "        early_stopping_threshold=0.001 # 0.001未満の改善は「改善なし」とみなす\n",
    "    )\n",
    "    # --- ここまで追加 ---\n",
    "\n",
    "    # ---- TrainingArguments ----\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=2,                 # 早期停止するため、ここは最大エポック数を指定\n",
    "        per_device_train_batch_size=128,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=wd,\n",
    "        warmup_ratio=warmup,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        \n",
    "        logging_steps=200,                  # 200ステップごとにログ記録\n",
    "        eval_strategy=\"steps\",        # <--- 追加: ステップごとに評価\n",
    "        eval_steps=200,                     # <--- 追加: 200ステップごとに評価（logging_stepsと合わせる）\n",
    "        save_strategy=\"steps\",              # <--- 変更: ステップごとに保存（評価と合わせる）\n",
    "        save_steps=200,                     # <--- 変更: 200ステップごとに保存\n",
    "        \n",
    "        load_best_model_at_end=True,        # <--- 追加: 学習終了時に最良モデルをロード\n",
    "        metric_for_best_model=\"eval_loss\",  # <--- 追加: 最良モデルの指標はeval_loss\n",
    "        greater_is_better=False,            # <--- 追加: lossは低い方が良い\n",
    "        save_total_limit=2,                 # チェックポイントは2つまで\n",
    "        \n",
    "        dataloader_num_workers=8,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        optim=\"schedule_free_radam\",\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        log_level=\"info\",\n",
    "    )\n",
    "\n",
    "    # ---- Trainer ----\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,   # <--- 変更: 訓練用データ\n",
    "        eval_dataset=eval_dataset,     # <--- 追加: 検証用データを追加\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[early_stopping_callback] # <--- 追加: 早期停止コールバック\n",
    "    )\n",
    "\n",
    "    # ---- 学習 ----\n",
    "    print(f\"\\n=== Training {exp_name} ===\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # ---- 最終メトリクス保存 ---- # <--- 変更\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"final_step\"] = trainer.state.global_step # 早期停止した場合のステップ数を記録\n",
    "    pd.DataFrame([metrics]).to_csv(f\"{output_dir}/final_training_metrics.csv\", index=False)\n",
    "\n",
    "    # ---- ステップごとの全ログ履歴を保存 ---- # <--- 追加\n",
    "    log_history_df = pd.DataFrame(trainer.state.log_history)\n",
    "    log_history_df.to_csv(f\"{output_dir}/full_log_history.csv\", index=False)\n",
    "\n",
    "    # ---- 保存 ----\n",
    "    # load_best_model_at_end=True のため、trainer.modelは既に検証ロス最小のモデル\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Saved best model (based on eval_loss) and tokenizer to {output_dir}\") # <--- 変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75493b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
